{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def get_environment():\n",
        "    try:\n",
        "        # Check if the environment is Google Colab\n",
        "        import google.colab\n",
        "        return \"colab\"\n",
        "    except ImportError:\n",
        "        # Environment is not Google Colab, assume it is local\n",
        "        return \"local\"\n",
        "\n",
        "environment = get_environment()\n",
        "\n",
        "if environment == \"colab\":\n",
        "    colab_path = \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    !pip install finance-datareader\n",
        "elif environment == \"local\":\n",
        "    colab_path = \"\""
      ],
      "metadata": {
        "id": "X5Sw1aQcBW7B",
        "outputId": "24efb382-df04-4c2a-af52-9b7000d2722b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting finance-datareader\n",
            "  Downloading finance_datareader-0.9.50-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from finance-datareader) (4.64.1)\n",
            "Collecting requests-file\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.8/dist-packages (from finance-datareader) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from finance-datareader) (2.23.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from finance-datareader) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.19.2->finance-datareader) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.19.2->finance-datareader) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.19.2->finance-datareader) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=0.19.2->finance-datareader) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.3.0->finance-datareader) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.3.0->finance-datareader) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.3.0->finance-datareader) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.3.0->finance-datareader) (1.24.3)\n",
            "Installing collected packages: requests-file, finance-datareader\n",
            "Successfully installed finance-datareader-0.9.50 requests-file-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hy305SdkBBfh"
      },
      "source": [
        "함수 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P8bI8YMGBBfk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import FinanceDataReader as fdr\n",
        "from datetime import timedelta\n",
        "\n",
        "# Neural Network library\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from keras import callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"bmh\")\n",
        "\n",
        "# 한글 폰트 사용을 위해서 세팅\n",
        "from matplotlib import font_manager, rc\n",
        "plt.rc('font', family='NanumGothic') # For Windows\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg3XcN35BBfl"
      },
      "source": [
        "## 2002-01-01 부터 학습하여 2018년 이후의 백테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KOPXfAEYBBfl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(colab_path + \"top50-Stock_LogReturn.csv\", index_col=\"Date\")\n",
        "train, test = train_test_split(df, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_len = len(train)\n",
        "test_len = len(test)\n",
        "validation_set_len = 80\n",
        "validation_set_split_point = 60\n",
        "\n",
        "xc_train = np.empty((train_len - validation_set_len, 60, 50))\n",
        "xf_train = np.empty((train_len - validation_set_len, 20, 50))\n",
        "xc_test = np.empty((test_len - validation_set_len, 60, 50))\n",
        "xf_test = np.empty((test_len - validation_set_len, 20, 50))\n",
        "\n",
        "for idx in range(train_len - validation_set_len):\n",
        "    temp_xc_train = train[idx : idx + validation_set_split_point]\n",
        "    temp_xf_train = train[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_train[idx] = temp_xc_train\n",
        "    xf_train[idx] = temp_xf_train\n",
        "\n",
        "for idx in range(test_len - validation_set_len):\n",
        "    temp_xc_test = test[idx : idx + validation_set_split_point]\n",
        "    temp_xf_test = test[idx + validation_set_split_point : idx + validation_set_len]\n",
        "\n",
        "    xc_test[idx] = temp_xc_test\n",
        "    xf_test[idx] = temp_xf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hb9wVW4WBBfm"
      },
      "outputs": [],
      "source": [
        "# 월간 수익률 정도의 스케일로 변환한다\n",
        "xc_train = xc_train.astype('float32') * 20\n",
        "xf_train = xf_train.astype('float32') * 20\n",
        "xc_test = xc_test.astype('float32') * 20\n",
        "xf_test = xf_test.astype('float32') * 20\n",
        "\n",
        "N_TIME = xc_train.shape[1]\n",
        "N_FUTURE = xf_train.shape[1]\n",
        "N_STOCKS = xf_train.shape[2]\n",
        "\n",
        "# 학습 데이터는 shuffling 한다.\n",
        "xc_train, xf_train = shuffle(xc_train, xf_train)\n",
        "\n",
        "# over confidence를 제어할 조절 변수 정의\n",
        "GAMMA_CONST = 0.1\n",
        "REG_CONST = 0.0\n",
        "\n",
        "# 최적 포트폴리오를 구축할 목표 함수를 정의한다.\n",
        "# MPN에서는 이 함수를 loss로 이용한다. max(objective) = min(-objective)\n",
        "# y_true = model.fit()에서 전달된 N_FUTURE일 후의 수익률 (xf_train)이 들어온다.\n",
        "# y_pred = 마코비츠 네트워크의 출력이 전달된다. (keras 내부 기능)\n",
        "\n",
        "def markowitz_objective(y_true, y_pred):\n",
        "    W = y_pred      # 마코비츠 네트워크의 출력\n",
        "    xf_rtn = y_true\n",
        "    W = tf.expand_dims(W, axis = 1)\n",
        "    R = tf.expand_dims(tf.reduce_mean(xf_rtn, axis = 1), axis = 2)\n",
        "    C = tfp.stats.covariance(xf_rtn, sample_axis=1)\n",
        "\n",
        "    rtn = tf.matmul(W, R)  \n",
        "    vol = tf.matmul(W, tf.matmul(C, tf.transpose(W, perm = [0, 2, 1]))) * GAMMA_CONST\n",
        "    reg = tf.reduce_sum(tf.square(W), axis = -1) * REG_CONST\n",
        "    objective = rtn - vol - reg\n",
        "    \n",
        "    return -tf.reduce_mean(objective, axis=0)\n",
        "\n",
        "# LSTM으로 Markowitz 모델을 생성한다.\n",
        "xc_input = tf.keras.Input(batch_shape = (None, N_TIME, N_STOCKS))\n",
        "h_lstm = LSTM(64, dropout = 0.5, kernel_regularizer=l2(0.005))(xc_input)\n",
        "y_output = Dense(N_STOCKS, activation='relu')(h_lstm)  # linear projection\n",
        "\n",
        "# 마코비츠의 최적 weights\n",
        "y_output = Activation('softmax')(y_output)\n",
        "model = tf.keras.Model(xc_input, y_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E45NTxs_BBfm"
      },
      "source": [
        "prophet이 계속 학습하면 계속 Loss가 줄어듦으로 에포크를 500으로 통일 제한한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk4-VnycBBfm",
        "outputId": "97500921-2911-4ad3-d848-8fd7d90c0026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "130/130 [==============================] - 9s 12ms/step - loss: 0.4058 - val_loss: 0.4039\n",
            "Epoch 2/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3944 - val_loss: 0.3926\n",
            "Epoch 3/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.3833 - val_loss: 0.3817\n",
            "Epoch 4/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3725 - val_loss: 0.3710\n",
            "Epoch 5/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3620 - val_loss: 0.3606\n",
            "Epoch 6/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3517 - val_loss: 0.3505\n",
            "Epoch 7/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.3417 - val_loss: 0.3406\n",
            "Epoch 8/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3319 - val_loss: 0.3309\n",
            "Epoch 9/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3224 - val_loss: 0.3215\n",
            "Epoch 10/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3130 - val_loss: 0.3123\n",
            "Epoch 11/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.3040 - val_loss: 0.3033\n",
            "Epoch 12/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.2950 - val_loss: 0.2945\n",
            "Epoch 13/500\n",
            "130/130 [==============================] - 1s 11ms/step - loss: 0.2864 - val_loss: 0.2859\n",
            "Epoch 14/500\n",
            "130/130 [==============================] - 1s 11ms/step - loss: 0.2779 - val_loss: 0.2776\n",
            "Epoch 15/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.2696 - val_loss: 0.2694\n",
            "Epoch 16/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2615 - val_loss: 0.2614\n",
            "Epoch 17/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2536 - val_loss: 0.2536\n",
            "Epoch 18/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2458 - val_loss: 0.2459\n",
            "Epoch 19/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2383 - val_loss: 0.2385\n",
            "Epoch 20/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2309 - val_loss: 0.2312\n",
            "Epoch 21/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.2236 - val_loss: 0.2240\n",
            "Epoch 22/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2166 - val_loss: 0.2171\n",
            "Epoch 23/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2097 - val_loss: 0.2102\n",
            "Epoch 24/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.2029 - val_loss: 0.2036\n",
            "Epoch 25/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1964 - val_loss: 0.1971\n",
            "Epoch 26/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1899 - val_loss: 0.1907\n",
            "Epoch 27/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1836 - val_loss: 0.1845\n",
            "Epoch 28/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1775 - val_loss: 0.1784\n",
            "Epoch 29/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1715 - val_loss: 0.1725\n",
            "Epoch 30/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1656 - val_loss: 0.1667\n",
            "Epoch 31/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1598 - val_loss: 0.1611\n",
            "Epoch 32/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1542 - val_loss: 0.1556\n",
            "Epoch 33/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1488 - val_loss: 0.1502\n",
            "Epoch 34/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1434 - val_loss: 0.1449\n",
            "Epoch 35/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1382 - val_loss: 0.1398\n",
            "Epoch 36/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.1330 - val_loss: 0.1349\n",
            "Epoch 37/500\n",
            "130/130 [==============================] - 1s 10ms/step - loss: 0.1280 - val_loss: 0.1301\n",
            "Epoch 38/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.1229 - val_loss: 0.1256\n",
            "Epoch 39/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1177 - val_loss: 0.1213\n",
            "Epoch 40/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1125 - val_loss: 0.1168\n",
            "Epoch 41/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1076 - val_loss: 0.1125\n",
            "Epoch 42/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.1029 - val_loss: 0.1081\n",
            "Epoch 43/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0984 - val_loss: 0.1039\n",
            "Epoch 44/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0940 - val_loss: 0.0999\n",
            "Epoch 45/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0898 - val_loss: 0.0959\n",
            "Epoch 46/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0858 - val_loss: 0.0920\n",
            "Epoch 47/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0819 - val_loss: 0.0883\n",
            "Epoch 48/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0782 - val_loss: 0.0846\n",
            "Epoch 49/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0745 - val_loss: 0.0811\n",
            "Epoch 50/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0710 - val_loss: 0.0776\n",
            "Epoch 51/500\n",
            "130/130 [==============================] - 1s 9ms/step - loss: 0.0675 - val_loss: 0.0743\n",
            "Epoch 52/500\n",
            "130/130 [==============================] - 2s 13ms/step - loss: 0.0642 - val_loss: 0.0710\n",
            "Epoch 53/500\n",
            "130/130 [==============================] - 1s 10ms/step - loss: 0.0610 - val_loss: 0.0678\n",
            "Epoch 54/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.0647\n",
            "Epoch 55/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0548 - val_loss: 0.0618\n",
            "Epoch 56/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0518 - val_loss: 0.0589\n",
            "Epoch 57/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0489 - val_loss: 0.0560\n",
            "Epoch 58/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0462 - val_loss: 0.0533\n",
            "Epoch 59/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0435 - val_loss: 0.0507\n",
            "Epoch 60/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0409 - val_loss: 0.0481\n",
            "Epoch 61/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0384 - val_loss: 0.0456\n",
            "Epoch 62/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0359 - val_loss: 0.0433\n",
            "Epoch 63/500\n",
            "130/130 [==============================] - 2s 12ms/step - loss: 0.0335 - val_loss: 0.0409\n",
            "Epoch 64/500\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0312 - val_loss: 0.0387\n",
            "Epoch 65/500\n",
            "130/130 [==============================] - 2s 16ms/step - loss: 0.0290 - val_loss: 0.0365\n",
            "Epoch 66/500\n",
            "130/130 [==============================] - 1s 8ms/step - loss: 0.0269 - val_loss: 0.0344\n",
            "Epoch 67/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0248 - val_loss: 0.0324\n",
            "Epoch 68/500\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 0.0229 - val_loss: 0.0305\n",
            "Epoch 69/500\n",
            "122/130 [===========================>..] - ETA: 0s - loss: 0.0202"
          ]
        }
      ],
      "source": [
        "# MPN을 학습하고 결과를 저장한다.\n",
        "SAVE_MODEL = 'Markowitz_network_final'\n",
        "ealry_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "model.compile(loss = markowitz_objective, optimizer = Adam(learning_rate = 1e-5))\n",
        "hist = model.fit(xc_train, xf_train, epochs=500, batch_size = 32, validation_data = (xc_test, xf_test), callbacks=[ealry_stopping])\n",
        "model.save(SAVE_MODEL + '.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dAC0DoOBBfn"
      },
      "outputs": [],
      "source": [
        "# loss trajectory를 확인한다.\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(hist.history['loss'], label='Train loss')\n",
        "plt.plot(hist.history['val_loss'], label='Test loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 최적 포트폴리오 결과 조회용 코드\n",
        "def check_w(n = 0):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    y_pred = model.predict(xc_test[n].reshape(1, N_TIME, N_STOCKS))[0]\n",
        "    plt.bar(np.arange(N_STOCKS), y_pred, alpha = 0.7)\n",
        "    plt.show()\n",
        "\n",
        "check_w(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtMLhW0wBBfn"
      },
      "source": [
        "MV 모델의 벤치마크"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaTopvZ8BBfn"
      },
      "outputs": [],
      "source": [
        "def calc_portfolio_std(weights, mean_returns, cov):\n",
        "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
        "    return portfolio_std\n",
        "\n",
        "def calc_neg_sharpe(weights, mean_returns, cov, rf):\n",
        "    portfolio_return = np.sum(mean_returns * weights)\n",
        "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
        "    sharpe_ratio = (portfolio_return - rf) / portfolio_std\n",
        "    return -sharpe_ratio\n",
        "\n",
        "def calc_portfolio_VaR(weights, mean_returns, cov, alpha):\n",
        "    portfolio_return = np.sum(mean_returns * weights)\n",
        "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
        "    portfolio_var = abs(portfolio_return - (portfolio_std * stats.norm.ppf(1 - alpha)))\n",
        "    return portfolio_return, portfolio_std, portfolio_var\n",
        "\n",
        "def portfolio_optimization_std(w, r, cov):\n",
        "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
        "    args = (r, cov)\n",
        "    bound = (0.0, 1.0)\n",
        "    bounds = tuple(bound for asset in range(len(w)))\n",
        "    optimization_result = minimize(calc_portfolio_std, w, args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return optimization_result\n",
        "\n",
        "def portfolio_optimization_sharpe(w, r, cov, rf):\n",
        "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
        "    args = (r, cov, rf)\n",
        "    bound = (0.0, 1.0)\n",
        "    bounds = tuple(bound for asset in range(len(w)))\n",
        "    optimization_result = minimize(calc_neg_sharpe, w, args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return optimization_result\n",
        "\n",
        "def portfolio_optimization_Var(w, r, cov, alpha):\n",
        "    constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}]\n",
        "    args = (r, cov, alpha)\n",
        "    bound = (0.0, 1.0)\n",
        "    bounds = tuple(bound for asset in range(len(w)))\n",
        "    optimization_result = minimize(calc_portfolio_VaR, w, args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
        "    \n",
        "    return optimization_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_P4ka7nBBfo"
      },
      "outputs": [],
      "source": [
        "N_TIME = xc_test.shape[1]\n",
        "N_FUTURE = xf_test.shape[1]\n",
        "N_STOCKS = xf_test.shape[2]\n",
        "\n",
        "# 저장된 Markowitz 모델을 가져온다.\n",
        "SAVE_MODEL = 'Markowitz_network_prophet_CV.h5'\n",
        "model = load_model(SAVE_MODEL, compile = True, custom_objects={'markowitz_objective': markowitz_objective})\n",
        "model.summary()\n",
        "\n",
        "# 백 테스트를 수행한다.\n",
        "prt_value = [10000]   # portfolio의 초기 value\n",
        "crp_value = [10000]   # CRP의 초기 value\n",
        "mvstd_value = [10000]    # MV_std의 초기 value\n",
        "mvshp_value = [10000] # MV_sharpe의 초기 value\n",
        "mvVaR_value = [10000] # MV_VaR의 초기 value\n",
        "w_crp = np.ones(N_STOCKS) / N_STOCKS   # CRP 비율 (균등 비율)\n",
        "\n",
        "w_history = []\n",
        "w_history_std = []\n",
        "w_history_shp = []\n",
        "w_history_VaR = []\n",
        "for i in range(0, xc_test.shape[0], N_FUTURE):\n",
        "   \n",
        "    # 이 시점에 각 종목을 w_prt 비율대로 매수한다.\n",
        "    # 학습할 때 월간 수익률로 변환했으므로, 여기서도 변환해야 한다.\n",
        "    x = xc_test[i][np.newaxis,:,:] * 20\n",
        "    w_prt = model.predict(x)[0]\n",
        "    w_history.append(w_prt)\n",
        "    \n",
        "    # 추가 코드\n",
        "    x_mean = np.mean(x[0], axis=0)\n",
        "    x_cov = np.cov(x[0].T)\n",
        "    \n",
        "    result_std = portfolio_optimization_std(w_crp, x_mean, x_cov)\n",
        "    w_std = np.array([round(x,3) for x in result_std['x']])\n",
        "    \n",
        "    result_sharpe = portfolio_optimization_sharpe(w_crp, x_mean, x_cov, 0)\n",
        "    w_shp = np.array([round(x,3) for x in result_sharpe['x']])\n",
        "    \n",
        "    result_VaR = portfolio_optimization_sharpe(w_crp, x_mean, x_cov, 0.05)\n",
        "    w_VaR = np.array([round(x,3) for x in result_VaR['x']])\n",
        "    \n",
        "    w_history_std.append(w_std)\n",
        "    w_history_shp.append(w_shp)\n",
        "    w_history_VaR.append(w_VaR)\n",
        "\n",
        "    # 다음 기간의 누적 수익률\n",
        "    m_rtn = np.sum(xf_test[i], axis = 0) / 20\n",
        " \n",
        "    # 누적 수익률과 w_prt (W)로 포트폴리오의 수익률을 계산한다.\n",
        "    prt_value.append(prt_value[-1] * np.exp(np.dot(w_prt, m_rtn)))\n",
        "    crp_value.append(crp_value[-1] * np.exp(np.dot(w_crp, m_rtn)))\n",
        "    \n",
        "    mvstd_value.append(mvstd_value[-1] * np.exp(np.dot(w_std, m_rtn)))\n",
        "    mvshp_value.append(mvshp_value[-1] * np.exp(np.dot(w_shp, m_rtn)))\n",
        "    mvVaR_value.append(mvVaR_value[-1] * np.exp(np.dot(w_VaR, m_rtn)))\n",
        "\n",
        "    # 추가로 발생한 시장 데이터로 MPN을 추가 학습시킨다.\n",
        "    # xc_test[0] ~ xc_test[19],\n",
        "    # xf_test[0] ~ xf_test[19]를 추가로 학습시킬 수 있다.\n",
        "    xc_new = xc_test[i:(i+N_FUTURE), :, :]\n",
        "    xf_new = xf_test[i:(i+N_FUTURE), :, :]\n",
        "\n",
        "    # xc_train 데이터에서 80개를 random sampling 한다.\n",
        "    idx = np.random.randint(0, xc_train.shape[0], 80)\n",
        "\n",
        "    # 추가 학습 데이터를 생성한다.\n",
        "    x = np.vstack([xc_new, xc_train[idx]])\n",
        "    y = np.vstack([xf_new, xf_train[idx]])\n",
        "    x, y = shuffle(x, y)\n",
        "\n",
        "    # 추가 학습한다.\n",
        "    model.fit(x, y, epochs=50, batch_size=10, verbose=0)\n",
        "\n",
        "    # 추가로 발생한 데이터도 이후에 sampling될 수 있도록 보관해 둔다.\n",
        "    xc_train = np.vstack([xc_train, xc_new])\n",
        "    xf_train = np.vstack([xf_train, xf_new])\n",
        "\n",
        "# 평가 시점의 날짜를 발췌한다.\n",
        "idx = np.arange(0, xc_test.shape[0] + 20, N_FUTURE)\n",
        "\n",
        "# 벤치마크를 위한 S&P500 정의\n",
        "us500 = fdr.DataReader('US500', '2002-01-01', '2022-12-01')\\\n",
        "    ['Adj Close'][-993:][idx]\n",
        "us500_value = us500 / us500[0] * 10000\n",
        "\n",
        "# Markowitz 성과와 CRP 성과를 데이터 프레임에 기록해 둔다.\n",
        "perf_df = pd.DataFrame({'crp':crp_value, 'markowitz':prt_value, 'MV_Std':mvstd_value, 'MV_Max_Shp':mvshp_value, 'MV_VaR':mvVaR_value,'S&P500':us500_value})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLwqboI2BBfo"
      },
      "source": [
        "누적 수익률 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2LcI3d0BBfo"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(perf_df)\n",
        "plt.tight_layout()\n",
        "plt.scatter(perf_df.index, perf_df['crp'])\n",
        "plt.scatter(perf_df.index, perf_df['markowitz'])\n",
        "plt.scatter(perf_df.index, perf_df['MV_Std'])\n",
        "plt.scatter(perf_df.index, perf_df['MV_Max_Shp'])\n",
        "plt.scatter(perf_df.index, perf_df['MV_VaR'])\n",
        "plt.scatter(perf_df.index, perf_df['S&P500'])\n",
        "plt.xticks(perf_df.index, rotation=90)\n",
        "plt.legend(perf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-DbQktcBBfp"
      },
      "source": [
        "MDD 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoUkqQxEBBfp"
      },
      "outputs": [],
      "source": [
        "perf_df_mdd = perf_df.copy()\n",
        "def mdd(df):\n",
        "    return (df.cummax() - df) / df.cummax() * 100\n",
        "\n",
        "temp_list = perf_df_mdd.columns\n",
        "for i in temp_list:\n",
        "    perf_df_mdd[i] = mdd(perf_df_mdd[i])\n",
        "    \n",
        "c = ['red', 'yellow', 'black', 'blue', 'orange', 'green']\n",
        "plt.title('모델 별 MDD')\n",
        "plt.bar(perf_df_mdd.columns, height=perf_df_mdd.max(), color=c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB4EtfIQBBfp"
      },
      "source": [
        "Std 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17ncFFOjBBfp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSsdJxixBBfp"
      },
      "source": [
        "샤프 비율 비교"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh868ukfBBfp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6RVcdViBBfp"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdFebOK1BBfq"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "finance",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "47ec0ad232d3e10df23e169c0b34d647f49874296b3dcb8d8db7cb70009d8222"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}